{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a8dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda env create -f env-win.yml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a9643",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda activate a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e26a696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from utils.gradcheck import gradcheck_naive\n",
    "from utils.utils import normalizeRows, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a78586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array.\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    s = 1. / (1. + np.exp(-x))\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff933ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveSoftmaxLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset\n",
    "):\n",
    "    \"\"\" Naive Softmax loss & gradient function for word2vec models\n",
    "\n",
    "    Implement the naive softmax loss and gradients between a center word's \n",
    "    embedding and an outside word's embedding. This will be the building block\n",
    "    for our word2vec models.\n",
    "\n",
    "    Arguments:\n",
    "    centerWordVec -- numpy ndarray, center word's embedding\n",
    "                    (v_c in the pdf handout)\n",
    "    outsideWordIdx -- integer, the index of the outside word\n",
    "                    (o of u_o in the pdf handout)\n",
    "    outsideVectors -- outside vectors (rows of matrix) for all words in vocab\n",
    "                      (U in the pdf handout)\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "\n",
    "    Return:\n",
    "    loss -- naive softmax loss\n",
    "    gradCenterVec -- the gradient with respect to the center word vector\n",
    "                     (dJ / dv_c in the pdf handout)\n",
    "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
    "                    (dJ / dU)\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### Please use the provided softmax function (imported earlier in this file)\n",
    "    ### This numerically stable implementation helps you avoid issues pertaining\n",
    "    ### to integer overflow. \n",
    "    gradCenterVec = np.zeros(outsideVectors.shape)\n",
    "    \n",
    "    z = np.dot(outsideVectors, centerWordVec)\n",
    "    y_hat = softmax(z)\n",
    "    loss = -np.log(y_hat[outsideWordIdx])\n",
    "    \n",
    "    y = np.zeros((outsideVectors.shape[0]))\n",
    "    y[outsideWordIdx] = 1\n",
    "    gradCenterVec = np.dot(outsideVectors.T, y_hat - y)\n",
    "    gradOutsideVecs = np.dot((y_hat - y).reshape((outsideVectors.shape[0],1)), centerWordVec.reshape((1,outsideVectors.shape[1])))\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15e7c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNegativeSamples(outsideWordIdx, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
    "\n",
    "    negSampleWordIndices = [None] * K\n",
    "    for k in range(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == outsideWordIdx:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        negSampleWordIndices[k] = newidx\n",
    "\n",
    "        return negSampleWordIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0b59f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negSamplingLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset,\n",
    "    K=1\n",
    "):\n",
    "    \"\"\" Negative sampling loss function for word2vec models\n",
    "\n",
    "    Implement the negative sampling loss and gradients for a centerWordVec\n",
    "    and a outsideWordIdx word vector as a building block for word2vec\n",
    "    models. K is the number of negative samples to take.\n",
    "\n",
    "    Note: The same word may be negatively sampled multiple times. For\n",
    "    example if an outside word is sampled twice, you shall have to\n",
    "    double count the gradient with respect to this word. Thrice if\n",
    "    it was sampled three times, and so forth.\n",
    "\n",
    "    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Negative sampling of words is done for you. Do not modify this if you\n",
    "    # wish to match the autograder and receive points!\n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    ### Please use your implementation of sigmoid in here.\n",
    "    gradOutsideVecs = np.zeros(outsideVectors.shape)\n",
    "    loss = 0\n",
    "    \n",
    "    z = sigmoid(np.dot(outsideVectors[outsideWordIdx], centerWordVec))\n",
    "    \n",
    "    loss += -np.log(z)\n",
    "    gradOutsideVecs[outsideWordIdx] = np.dot(z-1.0, centerWordVec)\n",
    "    gradCenterVec = np.dot(z - 1.0, outsideVectors[outsideWordIdx])\n",
    "    \n",
    "    for i in range(K):\n",
    "        w_k = indices[i+1]\n",
    "        z1 = sigmoid(-np.dot(outsideVectors[w_k], centerWordVec))\n",
    "        loss += -np.log(z1)\n",
    "        gradOutsideVecs[w_k] += np.dot(1.0 - z1, centerWordVec)\n",
    "        gradCenterVec -= np.dot(z1 - 1.0, outsideVectors[w_k])\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0875b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
    "             centerWordVectors, outsideVectors, dataset,\n",
    "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec\n",
    "\n",
    "    Implement the skip-gram model in this function.\n",
    "\n",
    "    Arguments:\n",
    "    currentCenterWord -- a string of the current center word\n",
    "    windowSize -- integer, context window size\n",
    "    outsideWords -- list of no more than 2*windowSize strings, the outside words\n",
    "    word2Ind -- a dictionary that maps words to their indices in\n",
    "              the word vector list\n",
    "    centerWordVectors -- center word vectors (as rows) for all words in vocab\n",
    "                        (V in pdf handout)\n",
    "    outsideVectors -- outside word vectors (as rows) for all words in vocab\n",
    "                    (U in pdf handout)\n",
    "    word2vecLossAndGradient -- the loss and gradient function for\n",
    "                               a prediction vector given the outsideWordIdx\n",
    "                               word vectors, could be one of the two\n",
    "                               loss functions you implemented above.\n",
    "\n",
    "    Return:\n",
    "    loss -- the loss function value for the skip-gram model\n",
    "            (J in the pdf handout)\n",
    "    gradCenterVecs -- the gradient with respect to the center word vectors\n",
    "            (dJ / dV in the pdf handout)\n",
    "    gradOutsideVectors -- the gradient with respect to the outside word vectors\n",
    "                        (dJ / dU in the pdf handout)\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
    "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    center_word_indx = word2Ind[currentCenterWord]\n",
    "    centerWordVec = centerWordVectors[center_word_indx]\n",
    "\n",
    "    for w in outsideWords: \n",
    "        _loss, _gradCenterVec, _gradOutsideVectors = word2vecLossAndGradient(centerWordVec, word2Ind[w], outsideVectors, dataset)\n",
    "        loss += _loss\n",
    "        gradCenterVecs[center_word_indx] += _gradCenterVec\n",
    "        gradOutsideVectors += _gradOutsideVectors\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVecs, gradOutsideVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a20bb3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Testing functions below. DO NOT MODIFY!   #\n",
    "#############################################\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, word2Ind, wordVectors, dataset,\n",
    "                         windowSize,\n",
    "                         word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    batchsize = 50\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    centerWordVectors = wordVectors[:int(N/2),:]\n",
    "    outsideVectors = wordVectors[int(N/2):,:]\n",
    "    for i in range(batchsize):\n",
    "        windowSize1 = random.randint(1, windowSize)\n",
    "        centerWord, context = dataset.getRandomContext(windowSize1)\n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerWord, windowSize1, context, word2Ind, centerWordVectors,\n",
    "            outsideVectors, dataset, word2vecLossAndGradient\n",
    "        )\n",
    "        loss += c / batchsize\n",
    "        grad[:int(N/2), :] += gin / batchsize\n",
    "        grad[int(N/2):, :] += gout / batchsize\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edc7be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sigmoid():\n",
    "    \"\"\" Test sigmoid function \"\"\"\n",
    "    print(\"=== Sanity check for sigmoid ===\")\n",
    "    assert sigmoid(0) == 0.5\n",
    "    assert np.allclose(sigmoid(np.array([0])), np.array([0.5]))\n",
    "    assert np.allclose(sigmoid(np.array([1,2,3])), np.array([0.73105858, 0.88079708, 0.95257413]))\n",
    "    print(\"Tests for sigmoid passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20186f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDummyObjects():\n",
    "    \"\"\" Helper method for naiveSoftmaxLossAndGradient and negSamplingLossAndGradient tests \"\"\"\n",
    "\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], \\\n",
    "            [tokens[random.randint(0,4)] for i in range(2*C)]\n",
    "\n",
    "    dataset = type('dummy', (), {})()\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "\n",
    "    return dataset, dummy_vectors, dummy_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "029430bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naiveSoftmaxLossAndGradient():\n",
    "    \"\"\" Test naiveSoftmaxLossAndGradient \"\"\"\n",
    "    dataset, dummy_vectors, dummy_tokens = getDummyObjects()\n",
    "\n",
    "    print(\"==== Gradient check for naiveSoftmaxLossAndGradient ====\")\n",
    "    def temp(vec):\n",
    "        loss, gradCenterVec, gradOutsideVecs = naiveSoftmaxLossAndGradient(vec, 1, dummy_vectors, dataset)\n",
    "        return loss, gradCenterVec\n",
    "    gradcheck_naive(temp, np.random.randn(3), \"naiveSoftmaxLossAndGradient gradCenterVec\")\n",
    "\n",
    "    centerVec = np.random.randn(3)\n",
    "    def temp(vec):\n",
    "        loss, gradCenterVec, gradOutsideVecs = naiveSoftmaxLossAndGradient(centerVec, 1, vec, dataset)\n",
    "        return loss, gradOutsideVecs\n",
    "    gradcheck_naive(temp, dummy_vectors, \"naiveSoftmaxLossAndGradient gradOutsideVecs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b421ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_negSamplingLossAndGradient():\n",
    "    \"\"\" Test negSamplingLossAndGradient \"\"\"\n",
    "    dataset, dummy_vectors, dummy_tokens = getDummyObjects()\n",
    "\n",
    "    print(\"==== Gradient check for negSamplingLossAndGradient ====\")\n",
    "    def temp(vec):\n",
    "        loss, gradCenterVec, gradOutsideVecs = negSamplingLossAndGradient(vec, 1, dummy_vectors, dataset)\n",
    "        return loss, gradCenterVec\n",
    "    gradcheck_naive(temp, np.random.randn(3), \"negSamplingLossAndGradient gradCenterVec\")\n",
    "\n",
    "    centerVec = np.random.randn(3)\n",
    "    def temp(vec):\n",
    "        loss, gradCenterVec, gradOutsideVecs = negSamplingLossAndGradient(centerVec, 1, vec, dataset)\n",
    "        return loss, gradOutsideVecs\n",
    "    gradcheck_naive(temp, dummy_vectors, \"negSamplingLossAndGradient gradOutsideVecs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b2539d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "def test_skipgram():\n",
    "    \"\"\" Test skip-gram with naiveSoftmaxLossAndGradient \"\"\"\n",
    "    dataset, dummy_vectors, dummy_tokens = getDummyObjects()\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, naiveSoftmaxLossAndGradient),\n",
    "        dummy_vectors, \"naiveSoftmaxLossAndGradient Gradient\")\n",
    "    grad_tests_softmax(skipgram, dummy_tokens, dummy_vectors, dataset)\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with negSamplingLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingLossAndGradient),\n",
    "        dummy_vectors, \"negSamplingLossAndGradient Gradient\")\n",
    "    grad_tests_negsamp(skipgram, dummy_tokens, dummy_vectors, dataset, negSamplingLossAndGradient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02b581c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#///////////////////////////////////////////\n",
    "#copied from utils/gradcheck.py file for skipgram test \n",
    "#//////////////////////////////////////////\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# First implement a gradient checker by filling in the following functions\n",
    "def gradcheck_naive(f, x, gradientText):\n",
    "    \"\"\" Gradient check for a function f.\n",
    "    Arguments:\n",
    "    f -- a function that takes a single argument and outputs the\n",
    "         loss and its gradients\n",
    "    x -- the point (numpy array) to check the gradient at\n",
    "    gradientText -- a string detailing some context about the gradient computation\n",
    "\n",
    "    Notes:\n",
    "    Note that gradient checking is a sanity test that only checks whether the\n",
    "    gradient and loss values produced by your implementation are consistent with\n",
    "    each other. Gradient check passing on its own doesnâ€™t guarantee that you\n",
    "    have the correct gradients. It will pass, for example, if both the loss and\n",
    "    gradient values produced by your implementation are 0s (as is the case when\n",
    "    you have not implemented anything). Here is a detailed explanation of what\n",
    "    gradient check is doing if you would like some further clarification:\n",
    "    http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/. \n",
    "    \"\"\"\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)\n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4        # Do not change this!\n",
    "\n",
    "    # Iterate over all indexes ix in x to check the gradient.\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        x[ix] += h # increment by h\n",
    "        random.setstate(rndstate)\n",
    "        fxh, _ = f(x) # evalute f(x + h)\n",
    "        x[ix] -= 2 * h # restore to previous value (very important!)\n",
    "        random.setstate(rndstate)\n",
    "        fxnh, _ = f(x)\n",
    "        x[ix] += h\n",
    "        numgrad = (fxh - fxnh) / 2 / h\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print(\"Gradient check failed for %s.\" % gradientText)\n",
    "            print(\"First gradient error found at index %s in the vector of gradients\" % str(ix))\n",
    "            print(\"Your gradient: %f \\t Numerical gradient: %f\" % (\n",
    "                grad[ix], numgrad))\n",
    "            return\n",
    "\n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print(\"Gradient check passed!. Read the docstring of the `gradcheck_naive`\"\n",
    "    \" method in utils.gradcheck.py to understand what the gradient check does.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def grad_tests_softmax(skipgram, dummy_tokens, dummy_vectors, dataset):\n",
    "    print (\"======Skip-Gram with naiveSoftmaxLossAndGradient Test Cases======\")\n",
    "\n",
    "    # first test\n",
    "    output_loss, output_gradCenterVecs, output_gradOutsideVectors = \\\n",
    "                skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "                dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
    "\n",
    "    assert np.allclose(output_loss, 11.16610900153398), \\\n",
    "           \"Your loss does not match expected loss.\"\n",
    "    expected_gradCenterVecs = [[ 0.,          0.,          0.        ],\n",
    "                               [ 0.,          0.,          0.        ],\n",
    "                               [-1.26947339, -1.36873189,  2.45158957],\n",
    "                               [ 0.,          0.,          0.        ],\n",
    "                               [ 0.,          0.,          0.        ]]\n",
    "    expected_gradOutsideVectors = [[-0.41045956,  0.18834851,  1.43272264],\n",
    "                                   [ 0.38202831, -0.17530219, -1.33348241],\n",
    "                                   [ 0.07009355, -0.03216399, -0.24466386],\n",
    "                                   [ 0.09472154, -0.04346509, -0.33062865],\n",
    "                                   [-0.13638384,  0.06258276,  0.47605228]]\n",
    "                     \n",
    "    assert np.allclose(output_gradCenterVecs, expected_gradCenterVecs), \\\n",
    "           \"Your gradCenterVecs do not match expected gradCenterVecs.\"\n",
    "    assert np.allclose(output_gradOutsideVectors, expected_gradOutsideVectors), \\\n",
    "           \"Your gradOutsideVectors do not match expected gradOutsideVectors.\"\n",
    "    print(\"The first test passed!\")\n",
    "\n",
    "    # second test\n",
    "    output_loss, output_gradCenterVecs, output_gradOutsideVectors = \\\n",
    "                skipgram(\"b\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "                dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
    "    assert np.allclose(output_loss, 9.87714910003414), \\\n",
    "           \"Your loss does not match expected loss.\"\n",
    "    expected_gradCenterVecs = [[ 0.,          0.,          0.        ],\n",
    "                               [-0.14586705, -1.34158321, -0.29291951],\n",
    "                               [ 0.,          0.,          0.        ],\n",
    "                               [ 0.,          0.,          0.        ],\n",
    "                               [ 0.,          0.,          0.        ]]\n",
    "    expected_gradOutsideVectors = [[-0.30342672,  0.19808298,  0.19587419],\n",
    "                                   [-0.41359958,  0.27000601,  0.26699522],\n",
    "                                   [-0.08192272,  0.05348078,  0.05288442],\n",
    "                                   [ 0.6981188,  -0.4557458,  -0.45066387],\n",
    "                                   [ 0.10083022, -0.06582396, -0.06508997]]\n",
    "                     \n",
    "    assert np.allclose(output_gradCenterVecs, expected_gradCenterVecs), \\\n",
    "           \"Your gradCenterVecs do not match expected gradCenterVecs.\"\n",
    "    assert np.allclose(output_gradOutsideVectors, expected_gradOutsideVectors), \\\n",
    "           \"Your gradOutsideVectors do not match expected gradOutsideVectors.\"\n",
    "    print(\"The second test passed!\")\n",
    "\n",
    "    # third test\n",
    "    output_loss, output_gradCenterVecs, output_gradOutsideVectors = \\\n",
    "                skipgram(\"a\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "                dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
    "\n",
    "    assert np.allclose(output_loss, 10.810758628593335), \\\n",
    "           \"Your loss does not match expected loss.\"\n",
    "    expected_gradCenterVecs = [[-1.1790274,  -1.35861865,  1.53590492],\n",
    "                               [ 0.,          0.,          0.        ],\n",
    "                               [ 0.,          0.,          0.        ],\n",
    "                               [ 0.,          0.,          0.        ],\n",
    "                               [ 0.,          0.,          0.        ]]\n",
    "    expected_gradOutsideVectors = [[-7.96035953e-01, -1.79609012e-02,  2.07761330e-01],\n",
    "                                   [ 1.40175316e+00,  3.16276545e-02, -3.65850437e-01],\n",
    "                                   [-1.99691259e-01, -4.50561933e-03,  5.21184016e-02],\n",
    "                                   [ 2.02560028e-02,  4.57034715e-04, -5.28671357e-03],\n",
    "                                   [-4.26281954e-01, -9.61816867e-03,  1.11257419e-01]]\n",
    "                                                     \n",
    "    assert np.allclose(output_gradCenterVecs, expected_gradCenterVecs), \\\n",
    "           \"Your gradCenterVecs do not match expected gradCenterVecs.\"\n",
    "    assert np.allclose(output_gradOutsideVectors, expected_gradOutsideVectors), \\\n",
    "           \"Your gradOutsideVectors do not match expected gradOutsideVectors.\"\n",
    "    print(\"The third test passed!\")\n",
    "\n",
    "    print(\"All 3 tests passed!\")\n",
    "    \n",
    "def grad_tests_negsamp(skipgram, dummy_tokens, dummy_vectors, dataset, negSamplingLossAndGradient):\n",
    "    print (\"======Skip-Gram with negSamplingLossAndGradient======\")  \n",
    "\n",
    "    # first test\n",
    "    output_loss, output_gradCenterVecs, output_gradOutsideVectors = \\\n",
    "                skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5,:],\n",
    "                dummy_vectors[5:,:], dataset, negSamplingLossAndGradient)\n",
    "\n",
    "    assert np.allclose(output_loss, 16.15119285363322), \\\n",
    "           \"Your loss does not match expected loss.\"\n",
    "    expected_gradCenterVecs = [[ 0.,          0.,          0.        ],\n",
    "                               [ 0.,          0.,          0.        ],\n",
    "                               [-4.54650789, -1.85942252,  0.76397441],\n",
    "                               [ 0.,          0.,          0.        ],\n",
    "                               [ 0.,          0.,          0.        ]]\n",
    "    expected_gradOutsideVectors = [[-0.69148188,  0.31730185,  2.41364029],\n",
    "                                   [-0.22716495,  0.10423969,  0.79292674],\n",
    "                                   [-0.45528438,  0.20891737,  1.58918512],\n",
    "                                   [-0.31602611,  0.14501561,  1.10309954],\n",
    "                                   [-0.80620296,  0.36994417,  2.81407799]]\n",
    "                     \n",
    "    assert np.allclose(output_gradCenterVecs, expected_gradCenterVecs), \\\n",
    "           \"Your gradCenterVecs do not match expected gradCenterVecs.\"\n",
    "    assert np.allclose(output_gradOutsideVectors, expected_gradOutsideVectors), \\\n",
    "           \"Your gradOutsideVectors do not match expected gradOutsideVectors.\"\n",
    "    print(\"The first test passed!\")\n",
    "\n",
    "    # second test\n",
    "    output_loss, output_gradCenterVecs, output_gradOutsideVectors = \\\n",
    "                skipgram(\"c\", 2, [\"a\", \"b\", \"c\", \"a\"], dummy_tokens, dummy_vectors[:5,:],\n",
    "                dummy_vectors[5:,:], dataset, negSamplingLossAndGradient)\n",
    "    assert np.allclose(output_loss, 28.653567707668795), \\\n",
    "           \"Your loss does not match expected loss.\"\n",
    "    expected_gradCenterVecs = [  [ 0.,          0.,          0.        ],\n",
    "                                 [ 0.,          0.,          0.        ],\n",
    "                                 [-6.42994865, -2.16396482, -1.89240934],\n",
    "                                 [ 0.,          0.,          0.        ],\n",
    "                                 [ 0.,          0.,          0.        ]]\n",
    "    expected_gradOutsideVectors = [  [-0.80413277,  0.36899421,  2.80685192],\n",
    "                                     [-0.9277269,   0.42570813,  3.23826131],\n",
    "                                     [-0.7511534,   0.34468345,  2.62192569],\n",
    "                                     [-0.94807832,  0.43504684,  3.30929863],\n",
    "                                     [-1.12868414,  0.51792184,  3.93970919]]\n",
    "                     \n",
    "    assert np.allclose(output_gradCenterVecs, expected_gradCenterVecs), \\\n",
    "           \"Your gradCenterVecs do not match expected gradCenterVecs.\"\n",
    "    assert np.allclose(output_gradOutsideVectors, expected_gradOutsideVectors), \\\n",
    "           \"Your gradOutsideVectors do not match expected gradOutsideVectors.\"\n",
    "    print(\"The second test passed!\")\n",
    "\n",
    "    # third test\n",
    "    output_loss, output_gradCenterVecs, output_gradOutsideVectors = \\\n",
    "                skipgram(\"a\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "                dummy_tokens, dummy_vectors[:5,:], \n",
    "                dummy_vectors[5:,:], dataset, negSamplingLossAndGradient)\n",
    "    assert np.allclose(output_loss, 60.648705494891914), \\\n",
    "           \"Your loss does not match expected loss.\"\n",
    "    expected_gradCenterVecs = [  [-17.89425315,  -7.36940626,  -1.23364121],\n",
    "                                 [  0.,           0.,           0.        ],\n",
    "                                 [  0.,           0.,           0.        ],\n",
    "                                 [  0.,           0.,           0.        ],\n",
    "                                 [  0.,           0.,           0.        ]]\n",
    "    expected_gradOutsideVectors = [[-6.4780819,  -0.14616449,  1.69074639],\n",
    "                                   [-0.86337952, -0.01948037,  0.22533766],\n",
    "                                   [-9.59525734, -0.21649709,  2.5043133 ],\n",
    "                                   [-6.02261515, -0.13588783,  1.57187189],\n",
    "                                   [-9.69010072, -0.21863704,  2.52906694]]\n",
    "                                                     \n",
    "    assert np.allclose(output_gradCenterVecs, expected_gradCenterVecs), \\\n",
    "           \"Your gradCenterVecs do not match expected gradCenterVecs.\"\n",
    "    assert np.allclose(output_gradOutsideVectors, expected_gradOutsideVectors), \\\n",
    "           \"Your gradOutsideVectors do not match expected gradOutsideVectors.\"\n",
    "    print(\"The third test passed!\")\n",
    "\n",
    "    print(\"All 3 tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c1d6c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sanity check for sigmoid ===\n",
      "Tests for sigmoid passed!\n",
      "==== Gradient check for naiveSoftmaxLossAndGradient ====\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "==== Gradient check for negSamplingLossAndGradient ====\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n",
      "======Skip-Gram with naiveSoftmaxLossAndGradient Test Cases======\n",
      "The first test passed!\n",
      "The second test passed!\n",
      "The third test passed!\n",
      "All 3 tests passed!\n",
      "==== Gradient check for skip-gram with negSamplingLossAndGradient ====\n",
      "Gradient check passed!. Read the docstring of the `gradcheck_naive` method in utils.gradcheck.py to understand what the gradient check does.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "\n",
    "def test_word2vec():\n",
    "    \"\"\" Test the two word2vec implementations, before running on Stanford Sentiment Treebank \"\"\"\n",
    "    test_sigmoid()\n",
    "    test_naiveSoftmaxLossAndGradient()\n",
    "    test_negSamplingLossAndGradient()\n",
    "    test_skipgram()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_word2vec()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520c39a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cd7641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
